%%This is a very basic article template.
%%There is just one section and two subsections.
\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref}
\usepackage{cite}
\usepackage{subfigure}
\usepackage{setspace}
\doublespacing

\addtolength{\oddsidemargin}{-.7in}
% \addtolength{\evensidemargin}{-2in}
\addtolength{\textwidth}{1.5in}
% 
\addtolength{\topmargin}{-1.2in}
\addtolength{\textheight}{2.3in}

\begin{document}


\title{Analyse and Prediction of Streaming
Application Package Timing Patterns on Multicore Machines\\
STAT248 Class Project}
\author{Dai Bui}
\maketitle
\section{Introduction}
Multicore machines have become more and more popular recently as a mechanism to
speed up applications while increasing clock frequency has become more and more
challenging. Now, applications are parallelized, partitioned and mapped into
several difference processors instead of running on a single processor. However, energy
saving is also one important aspect of the future computing research as there
are more and more computing devices produced each year that would consume a great
amount of energy. Streaming applications, i.e. MP3, MPEG4, e.t.c, is an
important class of applications, however, are not very difficult to parallelize.

In this project, we aim at using time series analsysis
techniques~\cite{BrillingerTimeSeries, ShumwayTimeSeries} to analyse and predict
timing interval patterns of packages sent between partitions of a streaming
applications on a multicore machine.

\section{System Model}
\subsection{System Architecture}
We assume a message passing multicore system composed of several processing
cores connected using a network on chip. Processing core (CPU) has its own
memory and is connected a router using a network interface (NI) as in
Figure~\ref{fig:MulticoreNoC}.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.7\columnwidth]{img/MulticoreNoC}
\caption{System architecture}\label{fig:MulticoreNoC}
\end{figure}

\subsection{Synchronous Dataflow Model of Computation}
One of the important feature of streaming applications that makes it possible to
predict the timing interval patterns of packages between partition is the
\textit{periodicity} semantic based on synchronous dataflow (SDF)~\cite{LeeSDF}
model of computation. Figure~\ref{fig:SDF} shows a graph of a simple SDF
application.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.5\columnwidth]{img/SDF}
\caption{An graph of an SDF application}\label{fig:SDF}
\end{figure}

In Figure~\ref{fig:SDF}, the SDF application is composed of three processes
called \textit{actors}. Each actor has multiple input and output ports. Each
input/output port is annotated with a number which is the number of
\textit{tokens} the port will consume/produce each time the actor containing the
port executes, so-called ~\textit{fires}. A token is a unit of data, it could be
a number, a package, e.t.c. Furthermore, for each channel, for instance, the
channel $D$ between $A$ and $B$, in order for data will not be accumulated on
the channel, actor $B$ has to fire twice as many as the actor $A$ fires because
actor $A$ will produce two tokens each time it fires and actor $B$ only consume
one token each time it fires. Similarly, actor $B$ has to fire three times as
many as the number of times actor $C$ fires. Therefore, an overall schedule of
actors for the SDF application will be $AAABBBBBBCC$ in one iteration. The
system could repeatedly execute the schedule again and again, and thus making
the execution of the system periodic.

\subsubsection{Actor Partitioning}
An SDF application is often composed of several actors and is partitioned into
several clusters and each cluster is then mapped to a core in a multi-core
machine. For example, in Figure~\ref{fig:SDF}, an SDF application can be partitioned into
two clusters in which the first cluster is composed of actors $A$ and $B$, the
second cluster is composed of actor $C$.  

\subsubsection{Actor Communication}
Data sent by actors located inside a core to another actor in another core are
first put into a buffer. When the buffer is full, data is sent to the
destination core. This accumulation mechanism could help increase the
\textit{silent} intervals between packages and thus reducing the number of time
to turn-on/off links and saving more power as turning links on/off consumes some
amount of power.


\section{Experiment}
\subsection{System Setup}
We implemented a simulation framework of a distributed memory multicore using
the SuperH processors~\cite{Stanley-MarbellSunflower} connected using a
cycle accurate Wormsim~\cite{Wormsim} network on chip simulator with Orion power
model~\cite{WangOrion}. We use Streamit benchmarks~\cite{ThiesStreamIt} that
respect SDF semantics to run on our simulator to obtain message trace and power
saving data.

\subsection{Package Timing Trace}
To evaluate intervals between packages, we first run our application to obtain a
network trace of messages. Based on that we use time series analysis
techniques~\cite{BrillingerTimeSeries, ShumwayTimeSeries} to build a statistical
model of messages based on that trace. The statistical model is then used to
predict the future message intervals.

\section{Timing Analysis}
\begin{figure}[ht!]
\centering
\includegraphics[width=0.7\columnwidth]{img/Cluster}
\caption{Package interval Autocorrelation}\label{fig:Cluster}
\end{figure}
We use an MP3 decoder application with actor graph as in
Figure~\ref{fig:Cluster}. The network trace is used is from cluster 4 to cluster
5. Figure~\ref{} shows timestamp packets sent between node 4 and 5, we then take
the difference of the timestamps of the packets to get the interval between
packets sent from cluster 4 to cluster 5 as in Figure~\ref{fig:s45} as the
project aims at predicting the intervals between packets. 

\begin{figure*}[ht!]
\subfigure[Package interval trace between clusters 4 and 5]{
\includegraphics[width=0.5\textwidth]{img/s45}
\label{fig:s45}
}
\centering
\subfigure[Package interval Autocorrelation]{
\includegraphics[width=0.45\textwidth]{img/acfs45}
\label{fig:acfs45}
}
\caption{Package Interval Sent Between Cluster 4 and 5}
\end{figure*}

% \begin{figure}[ht!]
% \centering
% \includegraphics[width=0.4\columnwidth]{img/s45}
% \caption{Package interval trace between clusters 4 and 5}\label{fig:s45}
% \end{figure}

\subsection{Autocovariance and Autocorrelation}
Auto covariance measures the linear dependence between two points in the same
series at different times.
\begin{equation}
\gamma(s,t)=cov(Y_s, Y_t)=E[(Y_s-\mu_s)(Y_t-\mu_t)]
\end{equation}
Autocorrelation measures the predictability of the series at time $t$, say $Y_t$
 using only value $Y_s$.
\begin{equation}
\rho(s,t)=\frac{\gamma(s,t)}{\sqrt{\gamma(s,s)\gamma(t,t)}}
\end{equation}

% \begin{figure}[ht!]
% \centering
% \includegraphics[width=0.4\columnwidth]{img/acfs45}
% \caption{Package interval Autocorrelation}\label{fig:acfs45}
% \end{figure}

\subsection{ARMA and Prediction}
Autoregressive moving average (ARMA)~\cite{BoxTimeSeries, FalkStatSAS}
 processes is used to predict the future values of a time series. We will first
 describe the basic of ARMA processes.
 
 An ARMA process is composition of an Autoregressive (AR) process and an Moving
 Average (MA) process. Let $(\epsilon_t)_{t\in \mathbb{Z}}$ be white noise,
 $\varphi_1, \ldots, \varphi_p, \theta_1, \ldots, \theta_q\in\mathbb{R}$.If a
stochastic process $(Y_t)_{t\in \mathbb{Z}}$ satisfies the following
 equation:
 \begin{equation}\label{eqn:ARMA}
 Y_t=\sum_{i=1}^{p}\varphi_iY_{t-i}+
 \epsilon_t +\sum_{i=1}^{q}\theta_i\epsilon_{t-i}
 \end{equation}
then the process $(Y_t)_{t\in \mathbb{Z}}$ is said to be
an \textit{autoregressive moving average process of order $p$, $q$}, denoted
ARMA($p$, $q$).

As we can see from the equation~\ref{eqn:ARMA}, if we can estimate the
parameters $\varphi_i$ and $\theta_i$ then we can \textit{predict} the future
outcome of an ARMA process based on previous observations of the process. To do
that, we use Kalman filter and one of the optimization methods to estimate the
\textit{maximum likelihood} given an observation sequence, the parameters
$\varphi_i$ and $\theta_i$ are most likely to be.

The equation~\ref{eqn:ARMA} first is converted into a state-space model (see
page 115 in~\cite{FalkStatSAS} ) to used in Kalman filter to estimate
likelihood. Then, Kalman filter can estimate the likelihood function
$f(Y_T,\ldots, Y_0|\varphi, \theta)$ which is the probability density
function of an observed sequence $(Y_t)_{t\in 1, \ldots, T}$ that given a set of
parameters $(\varphi, \theta)$.

Now we change the way saying about $f(Y_T,\ldots,Y_0|\varphi, \theta)$ that
given the set of observed data $(Y_t)_{t\in 1, \ldots, T}$, we want the likelihood of
some set of parameters $(\varphi, \theta)$, then we set:

\begin{equation}\label{eqn:parametersLikelihood}
\mathcal{L}(\varphi, \theta|Y_T,\ldots, Y_0) = f(Y_T,\ldots,Y_0|\varphi,
\theta)
\end{equation}

The equation~\ref{eqn:parametersLikelihood} suggests that we can use some
optimization methods like Powell and Nelder \& Mead methods, i.e. using
$\mathcal{L}(\varphi, \theta|Y_T,\ldots, Y_0)$ as the input, to search for the
maximum likelihood $\mathcal{L}(\varphi, \theta|Y_T,\ldots, Y_0)$, which means
the most suitable parameters.
\begin{equation}
(\hat{\varphi},\hat{\theta})=\arg\max_{\varphi, \theta} \mathcal{L}(\varphi,
\theta|Y_T,\ldots, Y_0)
\end{equation}

\subsubsection{Problem with Conventional ARMA}
Although ARMA is used to predict future values of a series based on
previously observed sequence of the series, the interval series of packages has
we use has a period of 8 as in the ACF in Figure~\ref{fig:acfs45}. This large
period makes it difficult to fit our series with an ARMA model since the order of the AR
part greater or at least equal to 8. The we could not make the fitting process
converge. To solve this problem, we use the Seasonal ARIMA (SARIMA).

\subsubsection{Seasonal ARIMA}
We model the input data as follow:
\begin{equation}
Y_t = \alpha Y_t+\beta Y_{t-8}+S_t+w_t
\end{equation}
where $S_t$ is the seasonal component and we again model it as a ARIMA process
with order (1,1,1), $w_t$ is white noise. The order (1,0,1) is used for the
seasonal, the fitting process does not converge. This indicates that the seasonal process is
non-stationary, which means that the seasonal component does not stay the same,
however, it changes slowly over time by a random walk.
\begin{equation}
S_t = S_{t-8}+v_t
\end{equation}
with $v_t$ is white noise and $w_t$ and $v_t$ are not correlated.

\subsection{Testing Residuals}
When we obtain the residuals from the difference between observation and
prediction data, it is our interest to test if the residuals is really white
noise or it still has some frequency components in it by comparing the smoothed
peridogram with confidence interval of white noise.

\subsubsection{Confidence Interval}
% \begin{equation}
% \hat{s}(f)=\sum_{k=-m}^mW
% \end{equation}
% 
% \begin{equation}
% var\{\hat{s} (f)\}=s(f)^2\sum_
% \end{equation}
From~\cite{bloomfieldFourierTimeSeries}, if $\hat{s}(f)$ is a spectral estimate
at frequency $f$: 
\begin{equation}
\hat{s}(f)=\sum_{k=-m}^mw_kI(f'-\frac{k}{n})
\end{equation}
where $f'$ is the Fourier frequency closest to $f$ and $n$ is the number length
of the series, $w_m$ are weights of the Daniel filter used to smooth the
peridogram.

and $s(f)$ is the true and unknown value of the spectrum, then:
\begin{equation}
var\{\hat{s}(f)\}=s(f)^2\sum_{k=-m}^mw_k^2
\end{equation}
Then
$\frac{\nu \hat{s}(f)}{s(f)}$ is approximately $\chi^2$ distributed with
$\nu=\sum_{k=-m}^mw_k^2$ is the degree of freedom. Therefore for an apprximate
$100(1-\alpha)\%$ confidence interval, we have:
\begin{equation}\label{eqn:confidenceInterval}
\frac{\nu\hat{s}(f)}{\chi_{\nu}^2(1-\alpha/2)}\leq s(f)\leq
\frac{\nu\hat{s}(f)}{\chi_{\nu}^2(\alpha/2)}
\end{equation}
which means that $100(1-\alpha)\%$ of true frequency false into the confidence
interval.
\subsubsection{White Noise Tesing}


\subsection{Correlation of Package Send/Receive Time}
\subsection{Conclusion}

\bibliographystyle{plain}
\bibliography{PkgTiming}

\end{document}
