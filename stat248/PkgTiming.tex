%%This is a very basic article template.
%%There is just one section and two subsections.
\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref}
\usepackage{cite}
\usepackage{subfigure}
\usepackage{setspace}
\doublespacing

\addtolength{\oddsidemargin}{-.7in}
% \addtolength{\evensidemargin}{-2in}
\addtolength{\textwidth}{1.5in}
% 
\addtolength{\topmargin}{-1.2in}
\addtolength{\textheight}{2.3in}

\begin{document}


\title{Analyse and Prediction of Streaming
Application Packet Timing Patterns on Multicore Machines\\
STAT248 Class Project}
\author{Dai Bui}
\maketitle
\section{Introduction}
Multicore machines have become more and more popular recently as a mechanism to
speed up applications while increasing clock frequency has become more and more
challenging. Now, applications are parallelized, partitioned and mapped into
several difference processors instead of running on a single processor. However, energy
saving is also one important aspect of the future computing research as there
are more and more computing devices produced each year that would consume a great
amount of energy. Streaming applications, i.e. MP3, MPEG4, e.t.c, is an
important class of applications, however, are not very difficult to parallelize.

In this project, we aim at using time series analysis
techniques~\cite{BrillingerTimeSeries, ShumwayTimeSeries} to analyse and predict
timing interval patterns of Packets sent between partitions of a streaming
applications on a multicore machine. This interval information is then used to
turning communication links off to save power if we predict that the interval
from a packet being sent to its successive packet in future is large. 

\section{System Model}
\subsection{System Architecture}
We assume a message passing multicore system composed of several processing
cores connected using a network on chip. Processing core (CPU) has its own
memory and is connected a router using a network interface (NI) as in
Figure~\ref{fig:MulticoreNoC}.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.6\columnwidth]{img/MulticoreNoC}
\caption{System architecture}\label{fig:MulticoreNoC}
\end{figure}

\subsection{Synchronous Dataflow Model of Computation}\label{sec:SDF}
One of the important feature of streaming applications that makes it possible to
predict the timing interval patterns of Packets between partition is the
\textit{periodicity} semantic based on synchronous dataflow (SDF)~\cite{LeeSDF}
model of computation. Figure~\ref{fig:SDF} shows a graph of a simple SDF
application.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.5\columnwidth]{img/SDF}
\caption{An graph of an SDF application}\label{fig:SDF}
\end{figure}

In Figure~\ref{fig:SDF}, the SDF application is composed of three processes
called \textit{actors}. Each actor has multiple input and output ports. Each
input/output port is annotated with a number which is the number of
\textit{tokens} the port will consume/produce each time the actor containing the
port executes, so-called ~\textit{fires}. A token is a unit of data, it could be
a number, a packet, and so on. We call the number annotated at an input/output
port the consumption/production rate. Furthermore, for each channel, for
instance, the channel $D$ between $A$ and $B$, in order for data will not be
accumulated on the channel, actor $B$ has to fire twice as many as the actor $A$
fires because actor $A$ will produce two tokens each time it fires and actor $B$
only consume one token each time it fires. Similarly, actor $B$ has to fire
three times as many as the number of times actor $C$ fires. Therefore, an
overall schedule of actors for the SDF application will be $AAABBBBBBCC$ in one
iteration. The system could repeatedly execute the schedule again and again, and
thus making the execution of the system periodic.

\subsubsection{Actor Partitioning}
An SDF application is often composed of several actors and is partitioned into
several disjoin clusters. Each cluster is then mapped to a processing core in a
multi-core machine. For example, in Figure~\ref{fig:comm}, an SDF application
can be partitioned into two clusters in which the first cluster is composed of
actors $A$ and $B$ runs in one core, the second cluster is composed of actor
$C$ runs in another core.

\subsubsection{Inter-cluster Communication}\label{sec:cluster_comm}
Data sent by actors located inside a cluster to actors in some other
cluster are first \textit{pushed} into a buffer as in Figure~\ref{fig:comm}. When the buffer
is full, data in the buffer is transmitted to another buffer located in the
destination cluster core. The actors in the destination clusters get data by
\textit{popping} data out from the receiving buffers. As a buffer size is
several times larger than a packet size, so when a buffer is sent, several
continuous packets are sent and it looks like a \textit{burst} of packets. This
accumulation mechanism could help increase the \textit{silent} intervals between
Packets and thus reducing the number of time to turn-on/off links and saving
more power as turning links on/off consumes some amount of power.


\begin{figure}[ht!]
\centering
\includegraphics[width=0.8\columnwidth]{img/comm}
\caption{System architecture}\label{fig:comm}
\end{figure}

\section{Experiment}
\subsection{System Setup}
We implemented a simulation framework of a distributed memory multicore using
the SuperH processors~\cite{Stanley-MarbellSunflower} connected using a
cycle accurate Wormsim~\cite{Wormsim} network on chip simulator with Orion power
model~\cite{WangOrion}. We use Streamit benchmarks~\cite{ThiesStreamIt} that
respect SDF semantics to run on our simulator to obtain message trace and power
saving data.

\subsection{Packet Timing Trace}
To evaluate intervals between Packets, we first run our application to obtain a
network trace of packets. Based on that we use time series analysis
techniques~\cite{BrillingerTimeSeries, ShumwayTimeSeries} to build a statistical
model of packets based on that trace. The statistical model is then used to
predict the future message intervals.

\section{Analysis}
\begin{figure}[ht!]
\centering
\includegraphics[width=0.5\columnwidth]{img/Cluster}
\caption{Clustering graph of an SDF application}\label{fig:Cluster}
\end{figure}

We use an MP3 decoder application with actor graph as in
Figure~\ref{fig:Cluster}. In the clustering graph, we see the \textit{push} and
\textit{pop} parameters in each cluster denoting how many times the actors in
the cluster push/pop data to/from actors in other clusters in one iteration
(one pass of schedule. See Section~\ref{sec:SDF}). The network trace is used is
from cluster 4 to cluster 5. Figure~\ref{fig:packettime} shows timestamps of
packets sent between node 4 and 5, we then take the difference of the timestamps
to get the intervals between successive packets sent from cluster 4 to cluster 5
as in Figure~\ref{fig:s45} as the project aims at predicting the intervals between packets.

\begin{figure*}[ht!]
\centering
\subfigure[Packet sending time]{
\includegraphics[width=0.45\textwidth]{img/packettime}
\label{fig:packettime}
}
\subfigure[Packet interval trace between clusters 4 and 5]{
\includegraphics[width=0.5\textwidth]{img/s45}
\label{fig:s45}
}
\caption{Packet Interval Sent Between Cluster 4 and 5}
\end{figure*}

% \begin{figure}[ht!]
% \centering
% \includegraphics[width=0.4\columnwidth]{img/s45}
% \caption{Packet interval trace between clusters 4 and 5}\label{fig:s45}
% \end{figure}
\begin{figure}[ht!]
\centering
\includegraphics[width=0.8\columnwidth]{img/s45All}
\caption{Original interval series}\label{fig:s45All}
\end{figure}

% \subsection{Autocovariance and Autocorrelation}
We first look at autocorrelation that measures the predictability of the series
at time $t$, say $Y_t$ using only value $Y_s$.
\begin{equation}
\rho(s,t)=\frac{\gamma(s,t)}{\sqrt{\gamma(s,s)\gamma(t,t)}}
\end{equation}

where $\gamma(s,t)$ is autocovariance which measures the linear dependence
between two points in the same series at different times.
\begin{equation}
\gamma(s,t)=cov(Y_s, Y_t)=E[(Y_s-\mu_s)(Y_t-\mu_t)]
\end{equation}
with $\mu_t=E[Y_t]$.

The autocorrelation of the series in Figure~\ref{fig:s45All} show strong
correlation between data points at lags that are multiple of 8. The raw
periodogram estimate of spectrums also shows high peaks at frequencies that are
multiple of $\frac{1}{8}$. Both of the above criteria exhibit that the interval
series has seasonal behaviour with period of 8. We compute the seasonal
components by computing the mean of data points at lag 8. The
Figure~\ref{fig:s45res} shows the residuals of the interval series after
removing the seasonal components. We can see that there is strong correlation
between data points at lag 64 and periodogram estimate peaks at frequencies
that are multiple of $\frac{1}{64}$. We also observe that, after removing
seasonal components, the residual series is \textit{sparse}, which means that
most of the series is zero, non-zero points only appear at the lag of 8. This
observation exhibits that the large intervals between two successive packets are
actually the interval between the last packet of a former \textit{burst} of data
(See Section~\ref{sec:cluster_comm}) and the first packet of its latter burst.
The intervals between packets in of a burst are constant. Therefore, now we only
consider the \textit{peak} intervals which are intervals that when we remove
the seasonal components, the residual is \textit{not} zero. For example, in
Figure~\ref{fig:s45All} peak intervals are points that are higher than the flat
bottom line.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.8\columnwidth]{img/s45res}
\caption{Residual series after removing seasonal}\label{fig:s45res}
\end{figure}



\subsection{ARMA and Prediction}
Autoregressive moving average (ARMA)~\cite{BoxTimeSeries, FalkStatSAS}
 processes is used to predict the future values of a time series. We will first
 describe the basic of ARMA processes.
 
 An ARMA process is composition of an Autoregressive (AR) process and an Moving
 Average (MA) process. Let $(\epsilon_t)_{t\in \mathbb{Z}}$ be white noise,
 $\varphi_1, \ldots, \varphi_p, \theta_1, \ldots, \theta_q\in\mathbb{R}$.If a
stochastic process $(Y_t)_{t\in \mathbb{Z}}$ satisfies the following
 equation:
 \begin{equation}\label{eqn:ARMA}
 Y_t=\sum_{i=1}^{p}\varphi_iY_{t-i}+
 \epsilon_t +\sum_{i=1}^{q}\theta_i\epsilon_{t-i}
 \end{equation}
then the process $(Y_t)_{t\in \mathbb{Z}}$ is said to be
an \textit{autoregressive moving average process of order $p$, $q$}, denoted
ARMA($p$, $q$).

As we can see from the equation~\ref{eqn:ARMA}, if we can estimate the
parameters $\varphi_i$ and $\theta_i$ then we can \textit{predict} the future
outcome of an ARMA process based on previous observations of the process. To do
that, we use Kalman filter and one of the optimization methods to estimate the
\textit{maximum likelihood} given an observation sequence, the parameters
$\varphi_i$ and $\theta_i$ are most likely to be.

The equation~\ref{eqn:ARMA} first is converted into a state-space model (see
page 115 in~\cite{FalkStatSAS} ) to used in Kalman filter to estimate
likelihood. Then, Kalman filter can estimate the likelihood function
$f(Y_T,\ldots, Y_0|\varphi, \theta)$ which is the probability density
function of an observed sequence $(Y_t)_{t\in 1, \ldots, T}$ that given a set of
parameters $(\varphi, \theta)$.

Now we change the way saying about $f(Y_T,\ldots,Y_0|\varphi, \theta)$ that
given the set of observed data $(Y_t)_{t\in 1, \ldots, T}$, we want the likelihood of
some set of parameters $(\varphi, \theta)$, then we set:

\begin{equation}\label{eqn:parametersLikelihood}
\mathcal{L}(\varphi, \theta|Y_T,\ldots, Y_0) = f(Y_T,\ldots,Y_0|\varphi,
\theta)
\end{equation}

The equation~\ref{eqn:parametersLikelihood} suggests that we can use some
optimization methods like Powell and Nelder \& Mead methods, i.e. using
$\mathcal{L}(\varphi, \theta|Y_T,\ldots, Y_0)$ as the input, to search for the
maximum likelihood $\mathcal{L}(\varphi, \theta|Y_T,\ldots, Y_0)$, which means
the most suitable parameters.
\begin{equation}
(\hat{\varphi},\hat{\theta})=\arg\max_{\varphi, \theta} \mathcal{L}(\varphi,
\theta|Y_T,\ldots, Y_0)
\end{equation}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.8\columnwidth]{img/s45peak}
\caption{Periodogram and ACF of peak intervals}\label{fig:s45peak}
\end{figure}

\subsubsection{Problem with Conventional ARMA}
Although ARMA is used to predict future values of a series based on
previously observed sequence of the series, the interval series of Packets has
we use has a period of 8 as in the ACF in Figure~\ref{fig:s45peak}. This large
period makes it difficult to fit our series with an ARMA model since the order of the AR
part greater or at least equal to 8. The we could not make the fitting process
converge. To solve this problem, we use the Seasonal ARIMA (SARIMA).

\begin{figure}[ht!]
\centering
\includegraphics[width=0.8\columnwidth]{img/tsdiag_fit}
\caption{Diagnostic of the fitted ARMA}\label{fig:tsdiag_fit}
\end{figure}

\subsubsection{Seasonal ARIMA}
We model the input data as follow:
\begin{equation}
%Y_t = \alpha T_{t-1}+\beta Y_{t-8}+S_t+w_t
Y_t = \varphi Y_{t-1}+S_t+ \theta w_{t-1}+w_t
\end{equation}
where $S_t$ is the seasonal component and $w_t$ is white noise. When order
(1,0,1) is used for the seasonal, the fitting process does not converge. This implies that the seasonal
process is non-stationary, which means that the seasonal does not
stay the same, however, it changes slowly over time by a random walk. To solve
this problem, we could differentiate the seasonal to make it stationary,
and thus, we use the order (0,1,1) for the seasonal with period of 8.
\begin{equation}
S_t - S_{t-8} = \alpha v_{t-1} + v_t
\end{equation}
with $v_t$ is white noise and $w_t$ and $v_t$ are not correlated. 
The AIC is about 1454.352 for other ARIMA orders of (2,0,1) or for the seasonal
of order (1,1,1). When we use the ARIMA order of (1,1,1) for the residuals, the
AIC is 1427.763, which is considerably difference, however, the prediction
result is worse visually. 

Use use the final model ARIMA(1,0,1)$\times$(0,1,1) with parameters  

$ar1=0.8127,ma1=0.2921, seasonal\_ma1=-0.657$

\begin{figure}[ht!]
\centering
\includegraphics[width=0.8\columnwidth]{img/predictions45}
\caption{Residuals after removing predicted values}\label{fig:predictions45}
\end{figure}

\subsection{Testing Residuals}
When we obtain the residuals from the difference between observation and
prediction data, it is our interest to test if the residuals is indeed white
noise or it still has some frequency components in it. We can see the
autocorrelation of the residuals in Figure~\ref{fig:predictions45}, there is no
significant correlation except at 0. This indicates that the residuals
potentially are white noise.

Another way to test if the residual series is white noise is by comparing the
smoothed periodogram estimate with confidence interval of white noise.

\subsubsection{Confidence Interval}
From~\cite{bloomfieldFourierTimeSeries}, if $\hat{s}(f)$ is a spectral estimate
at frequency $f$: 
\begin{equation}
\hat{s}(f)=\sum_{k=-m}^mw_kI(f'-\frac{k}{n})
\end{equation}
where $f'$ is the Fourier frequency closest to $f$ and $n$ is the number length
of the series, $w_m$ are weights of the Daniel filter used to smooth the
periodogram.

and $s(f)$ is the true and unknown value of the spectrum, then:
\begin{equation}
var\{\hat{s}(f)\}=s(f)^2\sum_{k=-m}^mw_k^2
\end{equation}
Then
$\frac{\nu \hat{s}(f)}{s(f)}$ is approximately $\chi^2$ distributed with
$\nu=\sum_{k=-m}^mw_k^2$ is the degree of freedom. Therefore for an approximate
$100(1-\alpha)\%$ confidence interval, we have:
\begin{equation}\label{eqn:confidenceInterval}
\frac{\nu\hat{s}(f)}{\chi_{\nu}^2(1-\alpha/2)}\leq s(f)\leq
\frac{\nu\hat{s}(f)}{\chi_{\nu}^2(\alpha/2)}
\end{equation}
which means that $100(1-\alpha)\%$ of true frequency false into the confidence
interval.
\subsubsection{White Noise Testing}
In the smoothed periodogram of the residuals in Figure~\ref{fig:predictions45},
the dashed and dot-dash curves mark the confidence interval of the smoothed
periodogram. We can see that there is now significant peaks in the periodogram
estimate. The value of the lower confidence interval of frequency is not clearly
larger then the periodogram estimate at other frequencies. This implies that the
is no significant statistical frequency in the residuals~\cite{CryerTimeSeries}.
We can also use the \textit{cross} to se the confidence interval of the smoothed
periodogram. The length of the vertical line is the range of confidence interval
at a given frequency in \textit{logarithm} scale. If we put the cross point of
the cross at a point at one periodogram estimate of a given frequency, then the
upper and lower ends of the vertical line mark the upper and lower confidence
interval in logarithm scale.

Another approach is that, given the understanding about confidence interval, we
now want to test if the residuals between predicted values and actual values are
actual white noise or not.

To do that, we first assume the residuals are white noise. Since the series is
white noise, then we then take the mean of the smoothed spectrum of the series
as the flat spectrum of the series, say $m$. From that flat spectrum of the
series, we then compute the upper and lower limits of the confidence interval:
\begin{align}
U &= \frac{\nu m}{\chi^2(\alpha/2)}\\
L &= \frac{\nu m}{\chi^2(100-\alpha/2)}
\end{align}

If we use confidence interval of $95\%$, then there should be no more than $5\%$
of the periodogram of the frequencies of the residual series fall outside of
that upper and lower limit lines.

In Figure~\ref{fig:predictions45}, the upper read line and lower green line
define the confidence interval range of the white noise with the periodogram
spectrum estimate is the mean of the residual periodogram spectrum estimates with
confidence 95\%. We can see that all the periodogram estimates of the residuals
fall the confidence interval range of white noise. This is another evidence
saying that the residuals are indeed white noise.

% \subsection{Correlation of Packet Send/Receive Times}
% To see how the packets are delayed in traffic, we test the cross-correlation
% between the time packets sent at source and the time packets received at
% destination.

\subsection{Result}
Figure~\ref{fig:results45} shows the result if predicted values in red. The
actual values lies in between the 95\% upper confidence, in blue, lower of
confidence,in green, limits. The confidence limits are computed as follows:
\begin{align}
upper\_limit = predicted\_value + 1.96*standard_error\\
lower\_limit = predicted\_value - 1.96*standard_error
\end{align}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.6\columnwidth]{img/results45}
\caption{Actual, predicted and confidence limit series}\label{fig:results45}
\end{figure}

\section{Conclusion and Future Work}
The main motivation for this project is to use ARMA technique to predict a
periodic process based on the observation that ARMA could be very suitable for
periodic processes and streaming applications based on SDF model of computation
is semantically periodic. We have showed that it possible to predict the
intervals between packets sent from one cluster to another cluster using ARMA.
However, in more dynamic SDF models of computation where
production/consumption rates of ports of actors could be changed resulting in
the change in the schedule, we need a more dynamic ARMA models. 

Using the data analysis methods, we could the see that most of the time, the
communication links are idle. A transmission link could be idle for millions of
cycles as showed in the analysis. During that time, if we could predict and turn
off the link, we could save the leakage power of the link. That could be a
considerable amount of power.

\bibliographystyle{plain}
\bibliography{PkgTiming}

\end{document}
